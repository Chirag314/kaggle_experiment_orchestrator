# orchestrator_agent/adk_tools.py

"""
Tools exposed to the LLM-based agent via google-genai's automatic
Python function calling.

We wrap our existing Python functions so that the agent can call them.
"""

from __future__ import annotations

from pathlib import Path

from .orchestrator import run_portfolio_analysis


def tool_run_portfolio_analysis(experiments_path=None):
    """
    Run portfolio analysis on the given experiments CSV.

    Parameters
    ----------
    experiments_path : str, optional
        Path to the experiments CSV. If not provided or empty, defaults to
        data/sample_experiments.csv in the project root.

    Returns
    -------
    dict
        Dictionary containing:
          - experiments_path
          - summary (dict)
          - text_report (str)
    """
    project_root = Path(__file__).resolve().parents[1]
    if not experiments_path:
        experiments_path = project_root / "data" / "sample_experiments.csv"
    else:
        experiments_path = Path(experiments_path)

    result = run_portfolio_analysis(experiments_path, verbose=False)
    return result


def tool_get_best_experiment(experiments_path=None):
    """
    Return a focused view on the best CV experiment.
    """
    result = tool_run_portfolio_analysis(experiments_path)
    best = result["summary"]["best_cv_experiment"]
    return best


def tool_get_overfitting_info(experiments_path=None):
    """
    Return information about the most overfitted experiment
    (largest cv_metric - holdout_metric gap).
    """
    result = tool_run_portfolio_analysis(experiments_path)
    worst = result["summary"]["worst_gap_experiment"]
    gap = worst["cv_metric"] - worst["holdout_metric"]
    return {
        "experiment": worst,
        "gap": gap,
    }


def tool_get_time_stats(experiments_path=None):
    """
    Return training time statistics and per-model time summary.
    """
    result = tool_run_portfolio_analysis(experiments_path)
    return {
        "time_stats": result["summary"]["time_stats"],
        "model_family_stats": result["summary"]["model_family_stats"],
    }


def tool_suggest_next_experiments(experiments_path=None):
    """
    Suggest next experiments to try based on:
      - best CV model
      - overfitting gap
      - model_family_stats
      - training time distribution

    Returns
    -------
    dict
      {
        "suggestions": [list of recommended experiments],
        "rationale": explanation text
      }
    """
    result = tool_run_portfolio_analysis(experiments_path)
    summary = result["summary"]

    best = summary["best_cv_experiment"]
    worst_gap = summary["worst_gap_experiment"]
    model_stats = summary["model_family_stats"]
    time_stats = summary["time_stats"]

    suggestions = []

    # 1. Fine-tune the best model
    suggestions.append(
        f"üìå Hyperparameter refinement on best model ({best['model_type']}): "
        "try tuning learning rate, max_depth, num_leaves, and regularization parameters."
    )

    # 2. Address overfitting
    if (best["cv_metric"] - best["holdout_metric"]) > 0.02:
        suggestions.append(
            "‚ö†Ô∏è High CV‚Äìholdout gap detected ‚Üí try stronger validation split (GroupKFold), "
            "feature regularization, or reduce model complexity."
        )
    else:
        suggestions.append(
            "üëç CV‚Äìholdout gap is reasonable ‚Üí explore additional features safely."
        )

    # 3. Try an alternative model family with strong mean CV
    sorted_models = sorted(
        model_stats.items(), key=lambda x: x[1]["mean_cv"], reverse=True
    )
    if len(sorted_models) > 1:
        next_best_model = sorted_models[1][0]
        suggestions.append(
            f"üîÅ Try second-best performing model family: {next_best_model} "
            "with improved feature engineering."
        )

    # 4. Speed-optimization experiment
    if time_stats["max_train_time"] > 2 * time_stats["mean_train_time"]:
        suggestions.append(
            "‚ö° Some models are slow ‚Üí build a 'fast baseline' (e.g., LightGBM with simple features) "
            "to accelerate iteration."
        )

    # 5. Feature engineering suggestion
    suggestions.append(
        "üß™ Add 1‚Äì2 new feature families (target encodings, polynomial interactions, "
        "frequency encodings, or domain-specific transformations)."
    )

    # 6. Ensemble recommendation
    suggestions.append(
        "ü§ù Build a small ensemble of top 2 model families (e.g., LightGBM + CatBoost). "
        "This often improves LB stability."
    )

    rationale = (
        "Recommendations are based on: best CV score, overfitting behavior, "
        "relative performance of model families, and training time statistics."
    )

    return {
        "suggestions": suggestions,
        "rationale": rationale,
    }


def tool_rank_experiments(experiments_path=None, strategy="balanced"):
    """
    Rank experiments and return a compact view (id, model_type, rank_score, cv, holdout, time).
    """
    from .tools import load_experiments  # local import to avoid cycles
    from .ranking import rank_experiments

    df = load_experiments(experiments_path)
    ranked = rank_experiments(df, strategy=strategy)

    cols = [
        "experiment_id",
        "model_type",
        "cv_metric",
        "holdout_metric",
        "train_time_seconds",
        "rank_score",
    ]
    existing_cols = [c for c in cols if c in ranked.columns]
    return ranked[existing_cols].to_dict(orient="records")
